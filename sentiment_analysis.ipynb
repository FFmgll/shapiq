{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'POSITIVE', 'score': 0.9885864853858948}]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(model=\"lvwerra/distilbert-imdb\", task=\"sentiment-analysis\")\n",
    "classifier(\"I have Never forgot this movie. All these years and it has remained in my life.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 1045, 2031, 2196, 9471, 2023, 3185, 1012, 2035, 2122, 2086, 1998, 2009, 2038, 2815, 1999, 2026, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.tokenizer(\"I have Never forgot this movie. All these years and it has remained in my\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You need to specify either `text` or `text_target`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtoken_to_word(batch_or_token_index\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m101\u001B[39m, \u001B[38;5;241m1045\u001B[39m, \u001B[38;5;241m2031\u001B[39m, \u001B[38;5;241m2196\u001B[39m, \u001B[38;5;241m9471\u001B[39m, \u001B[38;5;241m2023\u001B[39m, \u001B[38;5;241m3185\u001B[39m, \u001B[38;5;241m1012\u001B[39m, \u001B[38;5;241m2035\u001B[39m, \u001B[38;5;241m2122\u001B[39m, \u001B[38;5;241m2086\u001B[39m, \u001B[38;5;241m1998\u001B[39m, \u001B[38;5;241m2009\u001B[39m, \u001B[38;5;241m2038\u001B[39m, \u001B[38;5;241m2815\u001B[39m, \u001B[38;5;241m1999\u001B[39m, \u001B[38;5;241m2026\u001B[39m, \u001B[38;5;241m102\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\stable\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2514\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2512\u001B[0m all_kwargs\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[0;32m   2513\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 2514\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou need to specify either `text` or `text_target`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2516\u001B[0m     \u001B[38;5;66;03m# The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the\u001B[39;00m\n\u001B[0;32m   2517\u001B[0m     \u001B[38;5;66;03m# input mode in this case.\u001B[39;00m\n\u001B[0;32m   2518\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n",
      "\u001B[1;31mValueError\u001B[0m: You need to specify either `text` or `text_target`."
     ]
    }
   ],
   "source": [
    "classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'POSITIVE', 'score': 0.8662851452827454}]"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(model=\"lvwerra/distilbert-imdb\", task=\"sentiment-analysis\")\n",
    "classifier(\"I still remember watching Satya for the first time. I was completely blown away.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'NEGATIVE', 'score': 0.9796644449234009}]"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I rented this movie to get an easy, entertained view of the history of Texas. I got a headache instead.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class NLPGame:\n",
    "\n",
    "    def __init__(self, input_sentence):\n",
    "        self.classifier = pipeline(model=\"lvwerra/distilbert-imdb\", task=\"sentiment-analysis\")\n",
    "        self.tokenizer = self.classifier.tokenizer\n",
    "        self.tokenized_input = np.asarray(self.tokenizer(input_sentence)['input_ids'][1:-1])\n",
    "        self.input_sentence = self.tokenizer.decode(self.tokenized_input)\n",
    "        self.n = len(self.tokenized_input)\n",
    "        self.original_output = copy.copy(self.classifier(self.input_sentence))\n",
    "\n",
    "    def call(self, x):\n",
    "        outputs = self.classifier(x)\n",
    "        outputs = [output['score'] * 1 if output['label'] == 'POSITIVE' else output['score'] * -1 for output in outputs]\n",
    "        return outputs\n",
    "\n",
    "    def set_call(self, S):\n",
    "        x_inputs = []\n",
    "        for i, s in enumerate(S):\n",
    "            token_subset = self.tokenized_input[s]\n",
    "            x_text = self.tokenizer.decode(token_subset)\n",
    "            x_inputs.append(x_text)\n",
    "        return self.call(x_inputs)\n",
    "\n",
    "input_sent = \"I still remember watching Satya for the first time. I was completely blown away.\"\n",
    "nlp_game = NLPGame(input_sent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still time\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.8703174591064453]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_game.set_call([[1, 9]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[3148, 3666]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.asarray(nlp_game.tokenized_input)[[5,3]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "'i still remember watching satya for the first. i was completely blown away.'"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_game.tokenizer.decode([1045,\n",
    " 2145,\n",
    " 3342,\n",
    " 3666,\n",
    " 2938,\n",
    " 3148,\n",
    " 2005,\n",
    " 1996,\n",
    " 2034,\n",
    " 1012,\n",
    " 1045,\n",
    " 2001,\n",
    " 3294,\n",
    " 10676,\n",
    " 2185,\n",
    " 1012])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'POSITIVE', 'score': 0.8662851452827454}]"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_game.classifier(\"i still remember watching satya for the first time. i was completely blown away.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'POSITIVE', 'score': 0.8662851452827454}]"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_game.classifier(nlp_game.tokenizer.decode(nlp_game.tokenized_input[1:-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "'i still remember watching satya for the first time. i was completely blown away.'"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_game.tokenizer.decode(nlp_game.tokenized_input[1:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "pos_1 = \"I have Never forgot this movie. All these years and it has remained in my life.\"\n",
    "neg_1 = \"TWINS EFFECT is a poor film in so many respects. The only good element is that it doesn’t take itself seriously..\"\n",
    "neg_2 = \"I rented this movie to get an easy, entertained view of the history of Texas. I got a headache instead.\"\n",
    "neg_3 = \"Truly appalling waste of space. Me and my friend tried to watch this film to its conclusion but had to switch it off about 30 minutes from the end.\"\n",
    "pos_2 = \"I still remember watching Satya for the first time. I was completely blown away.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9996297359466553}\n",
      "{'label': 'POSITIVE', 'score': 0.9996439218521118}\n",
      "{'label': 'POSITIVE', 'score': 0.9811363816261292}\n",
      "{'label': 'POSITIVE', 'score': 0.9987378716468811}\n",
      "{'label': 'POSITIVE', 'score': 0.9996277093887329}\n",
      "{'label': 'POSITIVE', 'score': 0.9997320771217346}\n",
      "{'label': 'POSITIVE', 'score': 0.99965500831604}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analysis(\"I have Never forgot this movie. All these years and it has remained in my life.\")[0])\n",
    "print(sentiment_analysis(\"I Never forgot this movie. All these years and it has remained in my life.\")[0])\n",
    "print(sentiment_analysis(\"I have forgot this movie. All these years and it has remained in my life.\")[0])\n",
    "print(sentiment_analysis(\"I have Never this movie. All these years and it has remained in my life.\")[0])\n",
    "print(sentiment_analysis(\"I have Never forgot movie. All these years and it has remained in my life.\")[0])\n",
    "print(sentiment_analysis(\"I have Never forgot this All these years and it has remained in my life.\")[0])\n",
    "print(sentiment_analysis(\"I have Never forgot this movie. these years and it has remained in my life.\")[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sentiment_analysis(\"I have this movie. these years and it has remained in my life.\")[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9951967597007751}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analysis(\"I have this movie. these years and it has remained in my\")[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9996297359466553}\n",
      "{'label': 'NEGATIVE', 'score': 0.9997658133506775}\n",
      "{'label': 'NEGATIVE', 'score': 0.9977344274520874}\n",
      "{'label': 'NEGATIVE', 'score': 0.9998062252998352}\n",
      "{'label': 'NEGATIVE', 'score': 0.9991011619567871}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analysis(pos_1)[0])\n",
    "print(sentiment_analysis(neg_1)[0])\n",
    "print(sentiment_analysis(neg_2)[0])\n",
    "print(sentiment_analysis(neg_3)[0])\n",
    "print(sentiment_analysis(pos_2)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'POSITIVE', 'score': 0.9996297359466553}]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(pos_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from transformers import XLNetForSequenceClassification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c492e0877d35488099aa0286a15db4d0"
      },
      "application/json": {
       "n": 0,
       "total": 798011,
       "elapsed": 0.00500035285949707,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\anaconda3\\envs\\stable\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Max\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06892913b95f46c0b60a61aaad37531f"
      },
      "application/json": {
       "n": 0,
       "total": 760,
       "elapsed": 0.005007743835449219,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/467M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "138ddf1429d14a0dab501a84c12d92c9"
      },
      "application/json": {
       "n": 0,
       "total": 467042463,
       "elapsed": 0.0039899349212646484,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'LABEL_1'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "inputs = tokenizer(\"I have Never forgot this movie. All these years and it has remained in my life.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0674, 0.1090]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m predicted_class_id \u001B[38;5;241m=\u001B[39m \u001B[43mlogits\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_class_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4896, 0.5104]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "softmax(logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[0.4098, 0.5902]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "inputs = tokenizer(neg_1, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "class_id = model.config.id2label[predicted_class_id]\n",
    "softmax(logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3960, 0.6040]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(neg_3, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "class_id = model.config.id2label[predicted_class_id]\n",
    "softmax(logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}